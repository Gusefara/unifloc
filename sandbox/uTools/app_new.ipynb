{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кобзарь Олег, Хабибуллин Ринат, Буденный Семен, Адрианова Алла, Логинов Александр, Лемихов Александр\n",
    "# Обработчик данных \n",
    "\n",
    "В данной тетрадке собраны все средства для работы с данными (пре- и пост- процессор; генерация входных данных для модели, анализ рассчитанных значений, построение графиков)\n",
    "\n",
    "Большое количество кода вынесено в модуле - здесь в основном управление\n",
    "\n",
    "`well_name` - номер исследуемой скважины, например `1`\n",
    "\n",
    "Исходными данные лежат в папке  `data` в папках, соответствующим названиям скважин. Это данные\n",
    "1. Со станции управления (`data/well_name/well_name.csv`). Большое количество записей, которые нужно преобразовать в \"шахмоткоподобный\" вид для дальнейшей работы. Данные со СУ могут быть как высокочастотными (токи, напряжения), так и низкочастотными. \n",
    "2. С шахматки `data/well_name/Скв. well_name (01.07.2018-31.03.2019).xls`. Низкочастотные данные. \n",
    "3. C техрежима `data/tr/Техрежим, , февраль 2019.xls`. Какой насос спущен, ПЭД, на какую глубину. Эти данные постоянны на всем периоде расчета. В одном файлике один месяц и данные для всех скважин.\n",
    "\n",
    "\n",
    "\n",
    "## Общий workflow\n",
    "Здесь кратко, подробное описание будет над ячейками.\n",
    "1. Обработка исходных данных со СУ и приведение их в удобный формат. (init_edit)\n",
    "2. Считавание подготовленных ранее данных со СУ, шахматки, а затем генерации входных данных для модели, конкретно, адаптации модели скважины за выбранный период времени. (adaptation_input)\n",
    "3. Адаптация. Теперь можно открыть файл processor.py и в нем, выставив настройки и определив входные данные, запустить расчет скважину. После адаптации данные появится данные с калибровочными коэффициентами. (adaptation)\n",
    "4. Считывание рассчитанных значений адаптации, данных со СУ и шахматки. Генерации файлов для восстановления дебитов (restore_input)\n",
    "5. Восстановление. Работа в processor.py, где необходимо изменить метод расчета и указать на новые входные данные. (restore) \n",
    "6. Заключительный анализ: сведение результатов адаптации и восстановления; расчет различных метрик для оценки метода (тоже в папке restore)\n",
    "\n",
    "По идее, если данные уже сгенерированы, каждый пункт может повторяться многократно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт модулей. Некоторые функции и методы вынесены в отдельные `.py`\n",
    "* `preprocessor` - для обработки данных в и из модели\n",
    "* `processor` - для непосредственного запуска расчета адаптации или восстановления\n",
    "* `postprocessor` - для завершающего анализа сгенерированных данных. Определение метрик успешности модели, паттернов, событий, зависимостей.\n",
    "* `plotly_workflow` - для быстрого построения шаблонизированных графиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../../')\n",
    "import pandas as pd\n",
    "import unifloc.uniflocpy.uTools.plotly_workflow as pw\n",
    "import datetime\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot_workflow.plotly_option as pltl_opt\n",
    "import plot_workflow.plotly_workflow as pltl_wf\n",
    "\n",
    "from preproc_p import workflow_cs_data\n",
    "from preproc_p import workflow_chess_data\n",
    "from preproc_p import preproc_tool\n",
    "from preproc_p import workflow_calc_data\n",
    "from preproc_p import workflow_tr_data\n",
    "from preproc_p import filtration\n",
    "from proc_p import processor as proc\n",
    "\n",
    "from ml import calibr_restore as calibr_restore\n",
    "from postproc_p import result_and_metrics as result_and_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общие настройки и флаги для работы. Указание номера скважины и название файла шахматки. Флаги для запуска тех или иных ячеек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_name = '540'\n",
    "chess_file_name = 'Скв. ' + well_name + ' (01.08.2018-28.02.2019).xls'\n",
    "tr_name = \"Техрежим, , февраль 2019.xls\"\n",
    "# TODO убрать флаги\n",
    "read_initial_data = True\n",
    "plot_initial_data = True\n",
    "create_input_data = True\n",
    "multiprocessing_on = True #TODO может быть убрать флаги, как лучше?\n",
    "created_input_data_type = 0 # 0 - adaptation input, 1 - restore input with calibation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение путей к данным. Создание метки времени к генерируемым папкам. (чтоб данные не перезаписывались)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "time_mark = '' #datetime.datetime.today().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "path_to_data = current_path + \"\\\\data\\\\\"\n",
    "path_to_work_dir = current_path + \"\\\\data\\\\\" + well_name +  \"\\\\\"\n",
    "save_dir_name = 'init_edit'\n",
    "path_to_save = path_to_work_dir + save_dir_name + '\\\\'\n",
    "dirnames_list = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(path_to_data):\n",
    "    dirnames_list.extend(dirnames)\n",
    "    break\n",
    "print(dirnames_list)\n",
    "cs_data_filename = path_to_save + well_name + \"_first_edit.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание интервалов, на которых будет вестить расчет. Их может быть несколько. Рекомендуется поставить малый период времени, выявить ошибки, а затем запускать на месяцы и годы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_boundary = [datetime.datetime(2018,8,1), datetime.datetime(2018,11,29)]\n",
    "right_boundary = [datetime.datetime(2018,11,5), datetime.datetime(2019,2,28)]\n",
    "#left_boundary = [datetime.datetime(2018,8,3), datetime.datetime(2018,11,29),   datetime.datetime(2019,2,19)]\n",
    "#right_boundary = [datetime.datetime(2018,11,6), datetime.datetime(2019,2,4),  datetime.datetime(2019,2,28)]\n",
    "#left_boundary = [datetime.datetime(2019,1,30)]\n",
    "#right_boundary = [datetime.datetime(2019,2,28)]\n",
    "#left_boundary = [datetime.datetime(2018,8,1)]\n",
    "#right_boundary = [datetime.datetime(2018,12,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение функции которая все запустит в многопотоке и произведет из данного ноутбука адаптацию и восстановление. (Временный костыль - работает только здесь, в модуле не может)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_calculation(thread_option_list):\n",
    "    if __name__ == '__main__':\n",
    "        with Pool(amount_of_threads) as p:\n",
    "            p.map(proc.calc,\n",
    "                  thread_option_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Обработка исходных данных\n",
    "Чтение исходных данных `well_name.csv` со СУ и преобразование их в удобный формат. Процесс небыстрый из-за несовершенства алгоритма форматирования и количества данных (около 1-1,5 млн. записей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if read_initial_data:\n",
    "    try:\n",
    "        os.mkdir(path_to_work_dir + save_dir_name)\n",
    "    except:\n",
    "        pass\n",
    "    well_data = workflow_cs_data.read_and_edit_init_cs_data(well_name, path_to_work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение обработанных данных со СУ в папку `init_edit`\n",
    "\n",
    "Процесс может быть долгим - 3 минуты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#TODO использовать библиотеку modin вместо pandas\n",
    "well_data.to_csv(path_to_save + well_name + \"_first_edit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение графиков для нескольких интересующих параметров по исходным данных со СУ в `well_name__first_edit_report.html`. Процесс занимает около 3-5 минут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "well_data['ГФ, м3/м3'] = well_data['Объемный дебит газа'] / well_data['Объемный дебит нефти']\n",
    "all_banches = pltl_opt.create_banches_for_report(well_data, report_type = 'init_cs_data')\n",
    "pw.create_report_html(well_data, all_banches, path_to_save + well_name + \"_first_edit_report.html\")\n",
    "del well_data['ГФ, м3/м3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение исходных данных с шахматки в `well_name_chess_report.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chess_data = workflow_chess_data.load_and_edit_chess_data(path_to_work_dir + chess_file_name,\n",
    "                                     '1d', without_changing = True) #TODO сделать протяжку вверх, для буферного давления\n",
    "\n",
    "\n",
    "all_banches = pltl_opt.create_banches_for_report(chess_data, report_type = 'init_chess_data')\n",
    "pw.create_report_html(chess_data, all_banches, path_to_save + well_name + \"_chess_report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом этапе в папке init_edit окажутся\n",
    "1. well_name_init_edit.csv - данные с шахматки, которые будут подгружаться для адаптации и восстановления\n",
    "2. well_name_init_sp.html - график для исходных данных\n",
    "3.  well_name_init_edit_report.html - график для выбранных исходных данных\n",
    "\n",
    "Перейдем к следующему этапу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Генерация входных данных для адаптации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для данного этапа уже нужные обработанные данные со станции управления.\n",
    "\n",
    "Чтение исходных данных со СУ (в уже подготовленных). Обработка этих данных - ресемпл, пометка названий столбцов с помощью `(СУ)`. Т.к. данные высокочастотные, осредним их по 3 часам - примерному времени замера дебита скважины на АГЗУ. Обработка будет отличаться в зависимости от того, какие данные вы генерируете - для адаптации выбрасываются интервалы без замеров дебитов, для восстановления - нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_resamle = '3h'\n",
    "created_input_data_type = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "edited_data_cs = workflow_cs_data.load_and_edit_cs_data(cs_data_filename,\n",
    "                                       created_input_data_type,\n",
    "                                           time_to_resamle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведем фильтрацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_data_cs = filtration.get_filtred_by_sigma(edited_data_cs) #TODO добавить сюда фильтрацию по времени замера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение данных с шахматки. Осреднение и протяжка параметров. Пометка названий столбцов маркером `(Ш)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chess_data = workflow_chess_data.load_and_edit_chess_data(path_to_work_dir + chess_file_name,\n",
    "                                     time_to_resamle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация входных данных для адаптации. Сведение данных с шахматки и со СУ. Создание папки с названием `adapt_input_` и временной пометкой и помещение в нее:\n",
    "* сгенерированных входных данных для модели `well_name_adapt_input.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_dir_name = 'adapt_input_' + time_mark\n",
    "path_to_input_data = path_to_work_dir + input_data_dir_name + '\\\\'\n",
    "if create_input_data:\n",
    "    try:\n",
    "        os.mkdir(path_to_work_dir + input_data_dir_name)\n",
    "    except:\n",
    "        pass\n",
    "created_input_data = edited_data_cs.join(chess_data, how = 'inner') #TODO протягивать буферное давление вверх\n",
    "created_input_data = created_input_data.dropna(subset = ['Объемный дебит жидкости (СУ)'])\n",
    "created_input_data = preproc_tool.cut_df(created_input_data, left_boundary, right_boundary)\n",
    "created_input_data = preproc_tool.check_input_data(created_input_data)\n",
    "created_input_data.to_csv(path_to_input_data + well_name + '_adapt_input.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все входные данные на графике `well_name_adapt_input.html` в той же папке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file_path = path_to_input_data + well_name + '_adapt_input.html'\n",
    "input_data_traces = pw.create_traces_list_for_all_columms(created_input_data, 'lines+markers', use_gl = True)\n",
    "pw.plot_subplots(input_data_traces, plot_file_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение кривых в отчете только для тех данных, которые будут использоваться в модели в `well_name_adapt_input_report.html` в той же папке в стандартном виде. Можно настроить, какие данные выводить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_banches = pltl_opt.create_banches_for_report(created_input_data, report_type = 'adapt_input')\n",
    "\n",
    "pw.create_report_html(created_input_data, all_banches, path_to_input_data + well_name + '_adapt_input_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Адаптация\n",
    "Нужно, используя сгенерированные данные `well_name_adapt_input` получить значения коэффициентов калибровок по напору и мощности с помощью `processor.py`\n",
    "\n",
    "Для этого выключить флаги\n",
    "\n",
    "* vfm_calc_option = False\n",
    "* restore_q_liq_only = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка к параллельному расчету.\n",
    "\n",
    "В проекте UniflocVBA нужно вручную размножить надстройки до нужного количества потоков - каждая надстройка будет работать параллельно и рассчитывать определенную часть общих данных. \n",
    "\n",
    "По умолчанию выставлено 4 потока на 4 надстройках `UniflocVBA_7.xlam`, `UniflocVBA_7_1.xlam`, `UniflocVBA_7_2.xlam`, `UniflocVBA_7_3.xlam`. При желании можно добавить еще, не забыв изменить также номера потоков и их общее количество.\n",
    "\n",
    "Группировка информации о потоках в единый список `thread_option_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настройка многопоточности\n",
    "amount_of_threads = 12\n",
    "tr_name = \"Техрежим, , февраль 2019.xls\"\n",
    "dir_name_with_input_data = 'adapt_input_'\n",
    "\n",
    "thread_option_list =proc.create_thread_list(well_name, dir_name_with_input_data, tr_name,\n",
    "                       amount_of_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск процесса адаптации - получения калибровок. Процесс может быть долгим - в среднем расчет идет медленнее, чем при восстановлении дебитов. \n",
    "\n",
    "Рассчет 1 месяца в среднем занимает около 15-20 минут.\n",
    "\n",
    "Стоит отметить, что для аварийного выхода нужно вручную закрыть надстройки Excel (окна, которые открыты и в них ведется работа). После закрытия всех четырех процесс остановится и возникнет ошибка, контроль над jupyter notebook вернется и можно будет работать.\n",
    "\n",
    "Поэтому для тщательной отладки `processor.py` рекомендуется запускать отдельно (лучше через PyCharm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_calculation(thread_option_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Генерация данных для восстановления дебитов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Загрузка и анализ адаптации\n",
    "После успешной адаптации нужно проанализировать ее корректность, построив графики. Для этого нужно указать директории с \n",
    "* результатами адапатации - `adaptation_time_mark`  \n",
    "* входными данными для адаптации - `adaptation_input_time_mark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name_with_input_data = 'adapt_input_' + '\\\\'\n",
    "input_data_file_name = well_name + '_adapt_input'\n",
    "dir_name_with_calculated_data = 'adaptation_' + '\\\\'\n",
    "calculated_data_file_name = well_name + '_adapt_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если расчет проводился в многопоточном варианте, то загрузка будет отличаться. В папке с расчетами `adaptation` в сабдиректории `multiprocessing` будут результаты по частям в разных файликах, поэтому их нужно собрать в один файлик и поместить на уровень выше в `adaptation`. Если расчет был без использования многопоточности, этот шаг можно пропустить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multiprocessing_on == True:\n",
    "    first_result_data = preproc_tool.combine_multiprocessing_result(path_to_work_dir, dir_name_with_calculated_data)\n",
    "    first_result_data.to_csv(path_to_work_dir + dir_name_with_calculated_data + calculated_data_file_name + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка данных адаптации после расчета - в папке `adaptation_time_mark`  файл `well_name_adapt_1.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_data = workflow_calc_data.load_calculated_data_from_csv(path_to_work_dir + dir_name_with_calculated_data +\n",
    "                                                calculated_data_file_name +  '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение входных данных для адаптации из папки `adaptation_input` и объединение данных адатации и входных данных для нее.\n",
    "Сохранение всех данных в папке `adaptation_time_mark` с названием `well_name_calc_and_input.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv(path_to_work_dir + dir_name_with_input_data + input_data_file_name +  '.csv')\n",
    "input_data.index = input_data['Время']\n",
    "del input_data['Время']\n",
    "all_data = input_data.join(calculated_data, how = 'outer')\n",
    "\n",
    "all_data.to_csv(path_to_work_dir + dir_name_with_calculated_data + well_name + '_calc_and_input' +  '.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение графиков для данных адаптации в папке  `adaptation_time_mark`\n",
    "\n",
    "Все данные (входные и результаты) на одном графике `well_name_calc_and_input.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_data_traces = pw.create_traces_list_for_all_columms(all_data, 'lines+markers', use_gl = True)\n",
    "plot_file_path = path_to_work_dir + dir_name_with_calculated_data + well_name + '_calc_and_input' +  '.html'\n",
    "pw.plot_subplots(adapt_data_traces, plot_file_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание отчета - набор нужных графиков для быстрой оценки результатов адаптации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_banches = pltl_opt.create_banches_for_report(all_data, report_type = 'adapt_report')\n",
    "\n",
    "pw.create_report_html(all_data, all_banches, path_to_work_dir + dir_name_with_calculated_data + \n",
    "                      well_name + '_adapt_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Генерации данных для восстановления дебитов\n",
    "Работа с калибровками для восстановления дебитов. Для преобразования их выделим в отдельный DataFrame. Будем интерполировать, выкалавать точки, либо еще что-нибудь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibr_data = calculated_data[['К. калибровки по напору - множитель (Модель)',\n",
    "                               'К. калибровки по мощности - множитель (Модель)']]\n",
    "calibr_data = preproc_tool.mark_df_columns(calibr_data, 'Подготовленные')\n",
    "#calibr_data = calibr_data.resample('3h').mean()\n",
    "#calibr_data = calibr_data.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_out_lol, f_out_lol = calibr_restore.restore_calibr_via_ridge(all_data, calibr_data, use_80_20 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_resamle = '3h'\n",
    "created_input_data_type = 1 # костыль для 252 и 693 скважины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO учесть вариант, при котором калибровки линейно интерполируются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_data_cs = workflow_cs_data.load_and_edit_cs_data(cs_data_filename,created_input_data_type,\n",
    "                                       time_to_resamle\n",
    "                                       )\n",
    "chess_data = workflow_chess_data.load_and_edit_chess_data(path_to_work_dir + chess_file_name,\n",
    "                                     time_to_resamle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прозведем фильтрацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_data_cs = filtration.get_filtred_by_sigma(edited_data_cs) #TODO добавить сюда фильтрацию по времени замера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = preproc_tool.make_gaps_and_interpolate(calibr_data)\n",
    "calibr_data = result.copy()\n",
    "#all_data = all_data.join(calibr_data, how = 'inner')\n",
    "#created_input_data = all_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставляем предсказания машины\n",
    "calibr_data['К. калибровки по мощности - множитель (Модель) (Подготовленные)'] = p_out_lol\n",
    "calibr_data['К. калибровки по напору - множитель (Модель) (Подготовленные)'] = f_out_lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для прогноза на хвосте обрежем данные, чтобы считать только прогнозные часть - 20% от всего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibr_data = calibr_data[calibr_data['К. калибровки по мощности - множитель (Модель) (Подготовленные)']!=\n",
    "                          calculated_data['К. калибровки по мощности - множитель (Модель)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем данные со СУ и шахматки и обработаем их, для генерации входных данных модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация входных данных для модели для восстановления дебитов в папку `restore_input_time_mark`\n",
    "* `well_name_restore_input.csv` - входные данных с калибровками (увеличили частоту дискретизации или выкололи точки)\n",
    "* `well_name_restore_input.html` - все входные данные на одном графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_dir_name = 'restore_input_' + time_mark\n",
    "path_to_input_data = path_to_work_dir + input_data_dir_name + '\\\\'\n",
    "if create_input_data:\n",
    "    try:\n",
    "        os.mkdir(path_to_work_dir + input_data_dir_name)\n",
    "    except:\n",
    "        pass\n",
    "created_input_data = edited_data_cs.join(chess_data, how = 'inner')\n",
    "created_input_data = created_input_data.join(calibr_data, how ='inner')\n",
    "created_input_data = preproc_tool.cut_df(created_input_data, left_boundary, right_boundary)\n",
    "created_input_data.to_csv(path_to_input_data + well_name + '_restore_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file_path = path_to_input_data + well_name + '_restore_input.html'\n",
    "input_data_traces = pw.create_traces_list_for_all_columms(created_input_data, 'lines+markers', use_gl = True)\n",
    "pw.plot_subplots(input_data_traces, plot_file_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение графиков в отчетной форме "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_banches = pltl_opt.create_banches_for_report(created_input_data, report_type = 'restore_input')\n",
    "\n",
    "pw.create_report_html(created_input_data, all_banches, path_to_input_data  + \n",
    "                      well_name + '_restore_input_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Восстановление дебитов.\n",
    "Нужно, используя сгенерированные данные `well_name_restore_input` восстановить дебиты с помощью `postprocessor.py`\n",
    "Для этого активировать флаги\n",
    "* vfm_calc_option = True\n",
    "* restore_q_liq_only = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка к параллельному расчету.\n",
    "\n",
    "В проекте UniflocVBA нужно вручную размножить надстройки до нужного количества потоков - каждая надстройка будет работать параллельно и рассчитывать определенную часть общих данных. \n",
    "\n",
    "По умолчанию выставлено 4 потока на 4 надстройках `UniflocVBA_7.xlam`, `UniflocVBA_7_1.xlam`, `UniflocVBA_7_2.xlam`, `UniflocVBA_7_3.xlam`. При желании можно добавить еще, не забыв изменить также номера потоков и их общее количество.\n",
    "\n",
    "Группировка информации о потоках в единый список `thread_option_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настройка многопоточности\n",
    "\n",
    "amount_of_threads = 12\n",
    "tr_name = \"Техрежим, , февраль 2019.xls\"\n",
    "dir_name_with_input_data = 'restore_input_'\n",
    "\n",
    "thread_option_list =proc.create_thread_list(well_name, dir_name_with_input_data, tr_name,\n",
    "                       amount_of_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск процесса восстановления дебитов. Процесс может быть долгим - в среднем расчет восстановления дебитов идет быстрее, чем при адаптации (получении калибровок). \n",
    "\n",
    "Рассчет 1 месяца в среднем занимает около 10-15 минут.\n",
    "\n",
    "Стоит отметить, что для аварийного выхода нужно вручную закрыть надстройки Excel (окна, которые открыты и в них ведется работа). После закрытия всех четырех процесс остановится и возникнет ошибка, контроль над jupyter notebook вернется и можно будет работать.\n",
    "\n",
    "Поэтому для тщательной отладки processor.py рекомендуется его запускать отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_calculation(thread_option_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты расчета появятся в `well_name/restore_time_mark/multiprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Работа с данными после восстановления дебитов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Первичная обработка результатов восстановления\n",
    "Обозначим директории с результатами восстановления дебитов и входными данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name_with_input_data = 'restore_input_' + '\\\\'\n",
    "input_data_file_name = well_name +'_restore_input'\n",
    "dir_name_with_calculated_data = 'restore_' + '\\\\'\n",
    "calculated_data_file_name = well_name +'_restore_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединение данных при запуске расчета в многопотоке. Если расчет проводился без многопоточности, этот шаг можно пропустить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multiprocessing_on == True:\n",
    "    first_result_data = preproc_tool.combine_multiprocessing_result(path_to_work_dir, dir_name_with_calculated_data)\n",
    "    first_result_data.to_csv(path_to_work_dir + dir_name_with_calculated_data + calculated_data_file_name + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка данных после восстановления дебитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_data = workflow_calc_data.load_calculated_data_from_csv(path_to_work_dir + dir_name_with_calculated_data +\n",
    "                                                calculated_data_file_name +  '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединение входных и выходных данным модели. Сохранение результатов в `well_name_calc_and_input.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv(path_to_work_dir + dir_name_with_input_data + input_data_file_name +  '.csv', parse_dates = True, index_col = 'Время')\n",
    "all_data = input_data.join(calculated_data, how = 'outer')\n",
    "all_data.to_csv(path_to_work_dir + dir_name_with_calculated_data + well_name + '_calc_and_input' +  '.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение графиков восстановления дебитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_traces = pw.create_traces_list_for_all_columms(all_data, 'lines+markers', use_gl = True)\n",
    "plot_file_path = path_to_work_dir + dir_name_with_calculated_data + well_name + '_calc_and_input' +  '.html'\n",
    "pw.plot_subplots(input_data_traces, plot_file_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Сведение данных адаптации и восстановления\n",
    "Использование файлов типа `calc_and_input` в папках с адаптацией и восстновлением для формирования общего отчета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_adapt_dir = 'adaptation_' + '\\\\'\n",
    "path_to_restore_dir = 'restore_' + '\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка и слияние данных адаптации и восстановления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_data_with_input = pd.read_csv(path_to_work_dir + path_to_adapt_dir + well_name + '_calc_and_input' + '.csv' , parse_dates = True, index_col = 'Время')\n",
    "adapt_data_with_input = preproc_tool.mark_df_columns(adapt_data_with_input, 'ADAPT')\n",
    "restore_data_with_input = pd.read_csv(path_to_work_dir + path_to_restore_dir + well_name + '_calc_and_input' + '.csv' , parse_dates = True, index_col = 'Время')\n",
    "restore_data_with_input = preproc_tool.mark_df_columns(restore_data_with_input, 'RESTORE')\n",
    "overall_data = adapt_data_with_input.join(restore_data_with_input, how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение дебитов методом линейной интерполяции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_liq = overall_data[['Объемный дебит жидкости (СУ) (ADAPT)', 'Активная мощность (СУ) (ADAPT)']]\n",
    "result = preproc_tool.make_gaps_and_interpolate(q_liq)\n",
    "result = preproc_tool.mark_df_columns(result, 'INTERP')\n",
    "overall_data = overall_data.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заключительная обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_data = result_and_metrics.final_edit_overall_data(overall_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка вывода в отчет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_banches = pltl_opt.create_banches_for_report(overall_data, report_type = 'overall_result')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание сводного отчета `well_name_adapt_and_restore_report.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file_path = path_to_work_dir + path_to_restore_dir + well_name + '_adapt_and_restore_report' +  '.html'\n",
    "pw.create_report_html(overall_data, all_banches, plot_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расчет различных метрик для модели и сохранение их в текстовый файл `well_name_adapt_and_restore_metrics_report.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibr_calc_metrics, interp_calc_metrics = result_and_metrics.calc_calibr_interp_metrics(overall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_text_file_path = path_to_work_dir + path_to_restore_dir + well_name + '_adapt_and_restore_metrics_report' +  '.txt'\n",
    "text_file = open(metrics_text_file_path, \"w\")\n",
    "text_file.write(calibr_calc_metrics + '\\n' + interp_calc_metrics)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конец\n",
    "Основной цикл тетрадки закачивается. Дальше построение полезных графиков, их нужно распихать по модулям и в общий workflow. Тут шаблоны для построения корреляционной и тепловых карт в разном виде, построение напорной характеристики ЭНЦ, обезразмеревание величин"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Построение тепловой карты, характеристик и обезразмеренных величин\n",
    "Пару дополнительных функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import download_plotlyjs, plot, iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Указание имени файла с техрежимов (при ошибке чтения его нужно открыть и закрыть)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_name = \"Техрежим, , февраль 2019.xls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение данных с техрежима для данной скважины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_file_full_path = os.getcwd() + '\\\\data\\\\tr\\\\' + tr_name\n",
    "tr_data = workflow_tr_data.read_tr_and_get_data(tr_file_full_path, well_name)\n",
    "tr_data_df = pd.DataFrame({'Параметры скважины с ТР': list(tr_data.__dict__.values())})\n",
    "tr_data_df.index = list(tr_data.__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединение метрик и данных по скважине в единый DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_metrics = result_and_metrics.calc_calibr_interp_metrics(overall_data, return_df = True)\n",
    "\n",
    "overall_metrics['ЭЦН'] = [tr_data.esp_name_str, tr_data.esp_name_str]\n",
    "overall_metrics['ПЭД'] = [tr_data.motor_name_str, tr_data.motor_name_str]\n",
    "overall_metrics['Кол-во точек (ADAPT)'] = [len(overall_data['К. калибровки по напору - множитель (Модель) (ADAPT)']), \n",
    "                                           len(overall_data['К. калибровки по напору - множитель (Модель) (ADAPT)'])]\n",
    "\n",
    "overall_metrics.index = [well_name + ' (CALIBR)' , well_name + ' (INTERP)']\n",
    "overall_metrics.to_csv(path_to_work_dir + path_to_restore_dir + well_name + '_adapt_restore_metrics_tr_report' +  '.csv')\n",
    "overall_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение расширенных метрик для проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.read_csv(path_to_work_dir + path_to_restore_dir + well_name + '_adapt_restore_metrics_tr_report' +  '.csv', index_col = 'Unnamed: 0')\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызов UniflocVBA и построение напорных характеристик (по воде) данного насоса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_esp_nom_m3day = tr_data.esp_nom_rate_m3day\n",
    "head_esp_nom_m = tr_data.esp_nom_head_m\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import unifloc_vba.description_generated.python_api as python_api\n",
    "path_to_addin = os.getcwd()\n",
    "path_to_addin = path_to_addin.replace('unifloc\\\\sandbox\\\\uTools', 'unifloc_vba\\\\UniflocVBA_7.xlam')\n",
    "UniflocVBA = python_api.API(path_to_addin)\n",
    "h_m = []\n",
    "efficency_perc = []\n",
    "power_wt = []\n",
    "q_m3day = list(range(1, int(q_esp_nom_m3day * 1.8),10))\n",
    "pump_id = UniflocVBA.calc_ESP_id_by_rate(q_esp_nom_m3day)\n",
    "num_stages = int(head_esp_nom_m / UniflocVBA.calc_ESP_head_m(q_esp_nom_m3day, pump_id = pump_id))\n",
    "for i in q_m3day:\n",
    "    h_m.append(UniflocVBA.calc_ESP_head_m(i,num_stages = num_stages,  pump_id = pump_id))\n",
    "    power_wt.append(UniflocVBA.calc_ESP_power_W(i, num_stages = num_stages, pump_id = pump_id) / 1000)\n",
    "    efficency_perc.append(UniflocVBA.calc_ESP_eff_fr(i, num_stages = num_stages, pump_id = pump_id) * 100)\n",
    "esp_curve = pd.DataFrame({'Напор, м/100': h_m, 'Мощность, кВт': power_wt, 'КПД, %.': efficency_perc})\n",
    "esp_curve.index = q_m3day\n",
    "\n",
    "esp_curve.head()\n",
    "\n",
    "head_trace = pw.create_plotly_trace(q_m3day, h_m, 'Напор, м')\n",
    "power_trace = pw.create_plotly_trace(q_m3day, power_wt, 'Мощность, кВт')\n",
    "efficiency_trace = pw.create_plotly_trace(q_m3day, efficency_perc, 'КПД, %.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение параметров для анализа взаимосвязи между ними - df.corr() - коэффициента Парсона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nedeed_param_list = ['Q ж, м3/сут (Модель) (ADAPT)',\n",
    "                      'ГФ (Модель) (ADAPT)',\n",
    "                     'Обв, % (Модель) (ADAPT)',\n",
    "                     'К. калибровки по напору - множитель (Модель) (ADAPT)',\n",
    "                    'К. калибровки по мощности - множитель (Модель) (ADAPT)', \n",
    "                      'F тока, ГЦ (Модель) (ADAPT)',\n",
    "                     'Мощность, передаваемая СУ (Модель) (ADAPT)',\n",
    "                     'КПД ЭЦН, д.ед. (Модель) (ADAPT)',\n",
    "                     'Перепад давления в ЭЦН, атм (Модель) (ADAPT)',\n",
    "                     'P буф., атм (Модель) (ADAPT)',\n",
    "                     'Рлин ТМ (Ш) (ADAPT)',\n",
    "                     'Давление на приеме насоса (пласт. жидкость) (СУ) (ADAPT)'\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление взаимосвязи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_corr = overall_data[nedeed_param_list]\n",
    "all_data_corr = all_data_corr.corr()\n",
    "data=go.Heatmap(\n",
    "       z=all_data_corr.values,\n",
    "       x=list(all_data_corr.columns),\n",
    "       y=list(all_data_corr.index), showscale=False, colorscale = 'RdBu', zmid=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание обезразмеренного DataFrame для всех параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_data_dimensionless = result_and_metrics.make_dimensionless_df(overall_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание данных для plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_wc_gor = overall_data_dimensionless[['Q ж, м3/сут (Модель) (ADAPT)', 'ГФ (Модель) (ADAPT)', 'Обв, % (Модель) (ADAPT)']]\n",
    "q_wc_gor_trace = pw.create_traces_list_for_all_columms(q_wc_gor)\n",
    "calibrs = overall_data_dimensionless[['К. калибровки по напору - множитель (Модель) (ADAPT)', \n",
    "                                      'К. калибровки по мощности - множитель (Модель) (ADAPT)']]\n",
    "calibrs_trace = pw.create_traces_list_for_all_columms(calibrs)\n",
    "loss =  overall_data_dimensionless[['P буф., атм (Модель) (ADAPT)', \n",
    "                                      'Рлин ТМ (Ш) (ADAPT)',\n",
    "                                   'Мощность, передаваемая СУ (Модель) (ADAPT)']]\n",
    "loss_trace = pw.create_traces_list_for_all_columms(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация `well_name_dimless_pump_heatmap_report.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=7, cols=1, \n",
    "    specs=[[{\"type\": \"scattergl\"}], [{\"type\": \"scattergl\"}],\n",
    "           [{\"type\": \"scattergl\"}], [{\"type\": \"heatmap\"}],\n",
    "    [{\"type\": \"scattergl\"}], [{\"type\": \"scattergl\"}],\n",
    "    [{\"type\": \"scattergl\"}]]\n",
    ")\n",
    "fig.add_trace(q_wc_gor_trace[0],\n",
    "              row=1, col=1)\n",
    "fig.add_trace(q_wc_gor_trace[1],\n",
    "              row=1, col=1)\n",
    "fig.add_trace(q_wc_gor_trace[2],\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(calibrs_trace[0],\n",
    "              row=2, col=1)\n",
    "fig.add_trace(calibrs_trace[1],\n",
    "              row=2, col=1)\n",
    "\n",
    "fig.add_trace(loss_trace[0],\n",
    "              row=3, col=1)\n",
    "fig.add_trace(loss_trace[1],\n",
    "              row=3, col=1)\n",
    "fig.add_trace(loss_trace[2],\n",
    "              row=3, col=1)\n",
    "\n",
    "fig.add_trace(head_trace,\n",
    "              row=4, col=1)\n",
    "fig.add_trace(power_trace,\n",
    "              row=5, col=1)\n",
    "\n",
    "fig.add_trace(efficiency_trace,\n",
    "              row=6, col=1)\n",
    "\n",
    "fig.add_trace(data,\n",
    "              row=7, col=1)\n",
    "\n",
    "\n",
    "fig.update_layout(height=7 * 500,showlegend=True)\n",
    "plot(fig, filename = path_to_work_dir + path_to_restore_dir + well_name + '_dimless_pump_heatmap_report' +  '.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Сведение расчетов по скважинам: метрик и сводных тепловых карт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = [['1628','restore_2019_11_16_17_23_08', 'adaptation_2019_11_16_17_15_24'],\n",
    "         ['601', 'restore_2019_11_16_18_16_52', 'adaptation_2019_11_16_18_13_00'],\n",
    "         ['1602','restore_2019_11_15_20_03_23', 'adaptation_2019_11_15_19_49_55'],\n",
    "         ['1567','restore_2019_11_06_19_17_49', 'adaptation_2019_11_06_19_04_07'],\n",
    "         ['252','restore_2019_11_16_21_31_29', 'adaptation_2019_11_16_17_40_20'],\n",
    "         ['569', 'restore_2019_11_16_15_25_55', 'adaptation_2019_11_16_14_59_46'],\n",
    "         ['1354', 'restore_2019_11_15_15_13_07', 'adaptation_2019_11_15_14_48_55']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_names = ['1628', '601', '1602', '1567', '252', '569', '1354']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in wells:\n",
    "    path = current_path + \"\\\\data\\\\\" + i[0] + '\\\\' + i[1] + '\\\\' + i[0] + '_adapt_restore_metrics_tr_report.csv'\n",
    "    this_data = pd.read_csv(path, index_col = 'Unnamed: 0')\n",
    "    \n",
    "    \n",
    "    calc_path = current_path + \"\\\\data\\\\\" + i[0] + '\\\\' + i[1] + '\\\\' + i[0]\n",
    "    this_data_calc = pd.read_csv(calc_path + '_calc_and_input' + '.csv' , parse_dates = True, index_col = 'Время')\n",
    "    mean_val = this_data_calc['Q ж, м3/сут (Модель)'].mean()\n",
    "    \n",
    "    this_data['Q liq mean, m3day'] = [mean_val, mean_val]\n",
    "    \n",
    "    data.append(this_data)\n",
    "    \n",
    "start = data[0]\n",
    "for i in data[1:]:\n",
    "    start = start.append(i)\n",
    "\n",
    "start['MAE/Q liq mean, m3day'] = start['Mean absolute error']/start['Q liq mean, m3day']\n",
    "start.to_csv('overall_metrics.csv')\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = []\n",
    "for i in wells:\n",
    "    path = current_path + \"\\\\data\\\\\" + i[0] + '\\\\' + i[2] + '\\\\' + i[0] + '_calc_and_input.csv'\n",
    "    adapt_data_with_input = pd.read_csv(path, parse_dates = True, index_col = 'Время')\n",
    "    adapt_data_with_input = preproc_tool.mark_df_columns(adapt_data_with_input, 'ADAPT')\n",
    "    small_adapt_df = adapt_data_with_input[nedeed_param_list]\n",
    "    small_adapt_df_corr = small_adapt_df.corr()\n",
    "    corr_data.append(small_adapt_df_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_corr_data = {}\n",
    "for i in corr_data[0].columns:\n",
    "    this_df = corr_data[0].copy()\n",
    "    for j, k in zip(wells, corr_data) :\n",
    "        well_name = j[0]\n",
    "        this_df[well_name] = k[i]\n",
    "    this_df = this_df[well_names]\n",
    "    this_df = preproc_tool.mark_df_columns(this_df, '(скв)')\n",
    "    one_corr_data[i] = this_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heat_map_data(df, name):\n",
    "    data=go.Heatmap(\n",
    "           z=df.values,\n",
    "           x=list(df.columns),\n",
    "           y=list(df.index), showscale=False, colorscale = 'RdBu', zmid=0, name = name)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = create_heat_map_data(one_corr_data['Мощность, передаваемая СУ (Модель) (ADAPT)'],'Мощность, передаваемая СУ (Модель) (ADAPT)' )\n",
    "iplot([check])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap_report(one_corr_data):\n",
    "    len_all = len(one_corr_data.keys())\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=len_all, cols=1)\n",
    "    for i in range(len_all):\n",
    "        this_name = list(one_corr_data.keys())[i]\n",
    "        data = create_heat_map_data(one_corr_data[this_name], this_name)\n",
    "        this_row = i + 1\n",
    "        fig.add_trace(data,\n",
    "              row=this_row, col=1)\n",
    "    fig.update_layout(height=len_all * 500,showlegend=True)\n",
    "    plot(fig, filename ='heatmap_report' +  '.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap_report(one_corr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import download_plotlyjs, iplot\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_heatmap(df, filename_str):\n",
    "    corrs = df.corr()\n",
    "    figure = ff.create_annotated_heatmap(\n",
    "       z=corrs.values,\n",
    "       x=list(corrs.columns),\n",
    "       y=list(corrs.index),\n",
    "       annotation_text=corrs.round(2).values,\n",
    "       showscale=True)\n",
    "    figure.layout.update(\n",
    "        margin=go.layout.Margin(\n",
    "            l=400,\n",
    "            r=50,\n",
    "            b=100,\n",
    "            t=250\n",
    "        ))\n",
    "    filename_str +='.html'\n",
    "    plot(figure,filename=filename_str)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def plot_scatterplotmatrix(df, file_name):\n",
    "    figure = ff.create_scatterplotmatrix(df, diag='histogram')\n",
    "    figure.layout.update(width=2000, height=1500)\n",
    "    figure.layout.update(font=dict(size=7))\n",
    "    \n",
    "    plot(figure,filename=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_overall_data = overall_data[nedeed_param_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_heatmap(small_overall_data, 'small_overall_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimenless_small_overall_data = result_and_metrics.make_dimensionless_df(small_overall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimenless_small_overall_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatterplotmatrix(small_overall_data, 'small_overall_data_scatterplotmatrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
